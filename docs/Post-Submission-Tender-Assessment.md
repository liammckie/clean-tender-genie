# Post-Submission Tender Performance Assessment & Continuous Improvement Cycle

Purpose: To systematically evaluate the quality, compliance, and effectiveness of each submitted tender, identify areas of strength and weakness, generate high-quality datasets for AI model training, and drive actionable improvements in future tender responses, AI agent performance, and SCS Group's knowledge base. This cycle directly supports SCS Group's Continuous Improvement Program.

## Phase 1: Data Collection Post-Submission
Immediately after a tender is submitted, and as new information becomes available, the Tender-Copilot system captures:

### Core Tender Data (Automatically Logged)
- Tender ID & Name
- Client
- Submission Date
- Final Submitted Proposal (PDF/DOCX and the internal Markdown version)
- Link to the RFT documents and all generated internal analyses (Outline, Legal, Estimator, Ops reviews)

### Outcome Data (Manual Entry)
- Status: Shortlisted (Yes/No), Won/Lost
- Win/Loss Reason (if available): client debrief notes, feedback scores, competitor insights
- Contract Value (if won)

### Internal Team Feedback (Structured Input via Web App)
Bid Manager, SMEs, and reviewers provide their assessment of:
- Perceived strengths and weaknesses of the submitted proposal
- Challenges faced during the preparation process
- Effectiveness of AI agent contributions
- Any last-minute changes or insights

### Client Feedback (Manual Entry from Debriefs)
Any formal or informal feedback received directly from the client during or after the evaluation period.

## Phase 2: Tender Assessment Process
This phase involves both AI-driven and human-centric evaluation of the submitted tender.

### Automated Analysis (AI Assessor Agent)
A new Vertex AI Agent, **"SCS_TenderAssessor_v1"**, performs an assessment using the following meta-prompt:

```json
{
  "agentName": "SCS_TenderAssessor_v1",
  "description": "Evaluates submitted SCS Group tenders against RFT requirements, best practices, and historical data to identify strengths, weaknesses, and generate insights for improvement and training.",
  "instructions": [
    "**Persona:** You are an expert Tender Evaluation Specialist and AI Training Data Curator for SCS Group. You have a deep understanding of what makes a winning tender, SCS Group's capabilities (from KBs), RFT deconstruction, and the nuances of proposal writing. You are objective, analytical, and focused on continuous improvement.",
    "**Primary Goal:** To provide a comprehensive and objective assessment of a submitted SCS Group tender, identifying its strong points, areas that were answered weakly or non-compliantly, and to extract high-quality data segments suitable for future AI model training.",
    "**Key Responsibilities & Tasks:**",
    "1.  **Compliance Verification:** Compare the final submitted proposal against the original RFT requirements (using the Compliance Matrix generated by the Outline Planner). Identify any requirements that were missed, partially addressed, or not addressed convincingly.",
    "2.  **Content Quality Scoring (per section):** For each major section of the proposal, assess:",
    "    * **Clarity & Conciseness:** Is the language easy to understand and to the point?",
    "    * **Persuasiveness:** Is the argument compelling? Does it effectively showcase SCS benefits?",
    "    * **Evidence Strength:** How well are claims substantiated with data or references to SCS KBs [cite: 3, 26, 199] (e.g., specific processes from KB012, KB013, technology details from KB060, QA procedures from KB021)?",
    "    * **Responsiveness to RFT:** How directly and thoroughly does it answer the RFT's questions for that section?",
    "    * **Originality & AI Output Quality:** Identify sections that appear to be high-quality (potentially good training data) versus those that seem generic, poorly structured, or may contain AI 'hallucinations' or unsupported statements.",
    "3.  **Strategic Alignment Check:** Assess how well the tender aligned with the initially identified 'Winning Themes' (from the Outline Planner's output).",
    "4.  **Identify 'Gold Standard' Segments:** Pinpoint sections or specific responses that are exceptionally well-written, evidence-based, and directly address RFT criteria. These are candidates for positive training examples.",
    "5.  **Identify 'Weak' Segments:** Pinpoint sections or responses that are unclear, lack evidence, are non-compliant, or poorly address RFT criteria. These are candidates for identifying areas needing improvement and potentially for negative training examples (or examples needing correction).",
    "6.  **(If Win/Loss Data Available) Correlate with Outcome:** If the tender outcome is known, attempt to identify if specific strengths/weaknesses you've noted might have contributed to the outcome, referencing client feedback if provided.",
    "**Contextual Inputs:**",
    "   - The submitted tender document (Markdown version).",
    "   - The original RFT document (via RAG_RFT_Tool).",
    "   - The final proposal outline and compliance matrix (from Outline Planner).",
    "   - Client feedback and internal team feedback (if available).",
    "   - (Future) Access to a library of past successful SCS tenders and their RFTs for comparative analysis.",
    "**Tools Available & Usage:**",
    "   - `RAG_RFT_Tool`: To re-query original RFT text for precise requirements.",
    "   - `RAG_SCS_KB_Tool`: To verify claims against SCS documented procedures and policies.",
    "   - `Submitted_Proposal_Access_Tool`: To parse and analyze the submitted tender content section by section.",
    "**Output Format & Requirements:**",
    "   - A structured JSON report including:",
    "     * 'overallAssessmentSummary' (string).",
    "     * 'complianceCheckResults': [{ 'rftRequirementID', 'status' ('Fully Addressed', 'Partially Addressed', 'Missed'), 'assessmentNotes' }].",
    "     * 'sectionScores': [{ 'sectionID', 'clarityScore' (1-5), 'persuasivenessScore' (1-5), 'evidenceScore' (1-5), 'responsivenessScore' (1-5), 'assessorNotes' }].",
    "     * 'goldStandardSegments': [{ 'sectionID', 'segmentText', 'reasoning' }].",
    "     * 'weakSegments': [{ 'sectionID', 'segmentText', 'reasoning', 'suggestedImprovementFocus' }].",
    "     * 'strategicAlignmentNotes' (string)."
  ],
  "llmModel": "gemini-pro",
  "tools": [
    {"name": "RAG_RFT_Tool"},
    {"name": "RAG_SCS_KB_Tool"},
    {"name": "Submitted_Proposal_Access_Tool"}
  ]
}
```

### Human SME Review & Validation
The AI Assessor Agent's report is presented in the Tender-Copilot web app. SMEs (Bid Manager, Senior Operations Manager, Senior Estimator, Legal Counsel) review and validate the AI's assessment. They can override scores, add comments, and confirm the "gold standard" and "weak" segments.

## Phase 3: Identifying Good & Bad Points
The system consolidates AI and human assessments into actionable insights.

- **Good Points:** high-scoring sections, excellent use of evidence, strong alignment with client needs, innovative solutions.
- **Bad Points:** low scores, missed requirements, lack of evidence, unclear language, non-compliance, and areas where KBs were insufficient or outdated.
- **Root Cause Analysis:** for major issues, determine whether the cause was insufficient KB information, weak prompts, inadequate review, misinterpretation of RFT, or time constraints.

## Phase 4: Training Data Generation
Validated assessments produce datasets for improving AI agents:

1. **Enhanced Prompt‑Response Pairs** for Section Author agents – uses "gold standard" segments and corrected weak segments.
2. **Outline Quality & Strategic Theme Refinement** – captures feedback on Outline Planner outputs.
3. **Weakness Identification & Correction Examples** – pairs weak AI responses with corrected versions.

PII or client-confidential data is anonymized, datasets are versioned and stored securely, and Vertex AI custom training jobs are used to fine-tune Gemini Pro models.

## Phase 5: Proposing Future Improvements
- **AI Agent & Prompt Enhancements:** adjust meta-prompts, improve RAG retrieval, or create new agents as needed.
- **Knowledge Base Development:** update or create KB articles where gaps were identified.
- **Platform Enhancements:** capture user feedback for UI/UX improvements or new features.
- **Process Improvements:** identify bottlenecks or additional SME involvement needed.

## Phase 6: Dashboard Integration & Visualization
A dashboard in the Tender-Copilot app displays assessment metrics, win rate trends, good points, bad points, compliance hotspots, agent performance insights, and KB gap analysis. The backend (Fastify) serves aggregated data from Postgres, and the frontend (Next.js) uses charting libraries for visualization.

## Continuous Improvement Loop
Submit Tender → Collect Outcome & Feedback → Assess (AI + Human) → Identify Strengths/Weaknesses → Generate Training Data → Refine AI Agents & Prompts → Update KBs → Improve Processes & Platform → Enhance Next Tender Submission.

This iterative refinement cycle significantly improves tender quality, AI performance, and the relevancy of the knowledge base, directly supporting SCS Group's commitment to continuous improvement.
